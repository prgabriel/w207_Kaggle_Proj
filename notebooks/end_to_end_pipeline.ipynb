{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpack Kaggle Competition\n",
    "### W207 Final Project - Spring 2025\n",
    "\n",
    "Team: Perry Gabriel, Aurelia Yang\n",
    "\n",
    "University of California, Berkeley\n",
    "\n",
    "## Description\n",
    "\n",
    "In this competition, participants are challenged to develop machine learning models to predict the price of a backpack based on various features. This is a great opportunity to test your skills, learn new techniques, and compete with others in the data science community.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Submissions are evaluated on the root mean squared error between the predicted and actual price of the backpack.\n",
    "\n",
    "RMSE is defined as:\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "where $y_i$ is the actual price of the backpack and $\\hat{y}_i$ is the predicted price of the backpack.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data consists of the following columns:\n",
    "\n",
    "- `id`: A unique identifier for the backpack.\n",
    "- `Brand`: The brand of the backpack.\n",
    "- `Material`: The material of the backpack.\n",
    "- `Size`: The size of the backpack.\n",
    "- `Compartments`: The number of compartments in the backpack.\n",
    "- `Laptop Compartment`: Whether the backpack has a laptop compartment.\n",
    "- `Waterproof`: Whether the backpack is waterproof.\n",
    "- `Style`: The style of the backpack.\n",
    "- `Color`: The color of the backpack.\n",
    "- `Weight Capacity (kg)`: The weight capacity of the backpack in kilograms.\n",
    "- `Price`: The price of the backpack.\n",
    "\n",
    "## Data Splits\n",
    "The dataset is split into three parts:\n",
    "- **Train**: The training set contains 80% of the data and is used to train the model.\n",
    "- **Validation**: The validation set contains 10% of the data and is used to tune the model.\n",
    "- **Test**: The test set contains 10% of the data and is used to evaluate the model's performance.\n",
    "\n",
    "## Important Notes about the Dataset\n",
    "- There are (4) different datasets: train, train_extra, test, and sample_submission.\n",
    "- The `train` dataset contains the training data with the target variable `Price`.\n",
    "- The `train_extra` dataset contains additional training data that can be used to improve the model's performance.\n",
    "- The `test` dataset contains the test data without the target variable `Price`.\n",
    "- The `sample_submission` dataset contains a sample submission file with the correct format.\n",
    "- The `train` and `train_extra` datasets are combined to create a larger training set.\n",
    "- The `train_extra` dataset was provided by the competition organizers and is not part of the original dataset.\n",
    "\n",
    "## Submission File\n",
    "\n",
    "For each `id` in the test set, you must predict the price of the backpack. The file should contain a header and have the following format:\n",
    "\n",
    "```python\n",
    "id,Price\n",
    "1,100\n",
    "2,200\n",
    "3,300\n",
    "```\n",
    "\n",
    "## Timeline\n",
    "\n",
    "- **Start Date** - February 1, 2025\n",
    "- **Entry Deadline** - Same as the Final Submission Deadline\n",
    "- **Team Merger Deadline** - Same as the Final Submission Deadline\n",
    "- **Final Submission Deadline** - February 28, 2025\n",
    "\n",
    "All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This dataset was created by [Kaggle](https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset) for the purpose of hosting a competition.\n",
    "\n",
    "## Team Members\n",
    "\n",
    "- [Perry Gabriel](https://www.kaggle.com/prgabriel)\n",
    "- [Aurelia Yang](https://www.kaggle.com/aureliayang)\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. [Exploratory Data Analysis](#1.-Exploratory-Data-Analysis)\n",
    "2. [Data Preprocessing](#2.-Data-Preprocessing)\n",
    "3. [Modeling](#3.-Modeling)\n",
    "4. [Evaluation](#4.-Evaluation)\n",
    "5. [Optimization](#5.-Optimization)\n",
    "6. [Final Submission](#6.-Final-Submission)\n",
    "7. [Conclusion](#7.-Conclusion)\n",
    "\n",
    "## References\n",
    "[Backpack Kaggle Competition Link](https://www.kaggle.com/competitions/playground-series-s5e2)\n",
    "\n",
    "[Backpack Kaggle Competition Dataset](https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Install the required libraries\n",
    "Uncomment to download the data from Kaggle. This assumes you have the Kaggle API installed and configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c playground-series-s5e2\n",
    "# !unzip playground-series-s5e2 -d ../data/raw/\n",
    "# !pip install -r ../requirements.txt\n",
    "# !rm -rf playground-series-s5e2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) #used to supress the tf version warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(experiment_name='E2E_Kaggle_Backpack_Project')\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_import_colab():\n",
    "    global raw_data_path\n",
    "    global processed_path\n",
    "    try:\n",
    "        from google.colab import drive  \n",
    "        import google.colab\n",
    "        print(\"Running on Google Colab\")\n",
    "        # Import necessary libraries for Google Colab\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "        # define paths\n",
    "        raw_data_path = \"/content/drive/MyDrive/Kaggle_Backpack/data/raw/\"\n",
    "        processed_path = \"/content/drive/MyDrive/Kaggle_Backpack/data/processed/\"\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Not running on Google Colab\")\n",
    "        os.makedirs(raw_data_path, exist_ok=True)\n",
    "        print(\"Created 'raw_data_path' directory for non-Colab Environment.\")\n",
    "        return False\n",
    "\n",
    "on_colab = check_and_import_colab()\n",
    "\n",
    "if on_colab:\n",
    "    print(\"Google Colab environment detected. Paths have been set accordingly.\")\n",
    "else:\n",
    "    print(\"Local environment detected. Paths have been set accordingly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "In this section, we will explore the data to understand its structure and identify any patterns or trends that may be present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Data\n",
    "\n",
    "Let's start by loading the data and taking a look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(filepath_or_buffer=os.path.join(raw_data_path, 'train.csv'), index_col=0, header=0, sep=',')\n",
    "test_df = pd.read_csv(filepath_or_buffer=os.path.join(raw_data_path, 'test.csv'), index_col=0, header=0, sep=',')\n",
    "train_extra_df = pd.read_csv(filepath_or_buffer=os.path.join(raw_data_path, 'training_extra.csv'), index_col=0, header=0, sep=',')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Summary\n",
    "\n",
    "Next, let's take a look at the summary statistics of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary statistics of the training data\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data types of columns in training dataset\\n{train_df.dtypes}\\n\")\n",
    "print(f\"Data types of columns in training extra dataset\\n{train_extra_df.dtypes}\\n\")\n",
    "print(f\"Data types of columns in testing dataset\\n{test_df.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset.\n",
    "print(f\"Shape of training data: {train_df.shape}\")\n",
    "print(f\"Shape of training extra data: {train_extra_df.shape}\")\n",
    "print(f\"Shape of testing data: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the train and train_extra datasets\n",
    "\n",
    "Now, let's combine the train and train_extra datasets to create a larger training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and train_extra datasets\n",
    "train_df = pd.concat([train_df, train_extra_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Display the shape of the combined dataset\n",
    "print(f\"Shape of combined training data: {train_df.shape}\")\n",
    "\n",
    "# Display the shape of the combined dataset\n",
    "train_df.shape\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture the categories of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = train_df.columns[:-2].tolist()  # Dropped the last two columns since we know that these are numerical columns\n",
    "print(f'There are {len(cat_columns)} categorical columns:')\n",
    "print(cat_columns)\n",
    "\n",
    "num_columns = [train_df.columns[-2]]\n",
    "print(f'There are {len(num_columns)} numerical column:')\n",
    "print(num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Visualization\n",
    "\n",
    "We created visualizations to better understand the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, plot a histogram of the price column\n",
    "plt.hist(train_df['Price'], bins=20, edgecolor='black', color='skyblue', rwidth=0.8)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Price (train_df)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Correlation Matrix\n",
    "\n",
    "Finally, let's create a correlation matrix to see how the features are related to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns\n",
    "numeric_cols = train_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr = numeric_cols.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations so far: \n",
    "\n",
    "- `training_extra` has significantly more records (3.69M) than `training` (300k), which will be useful in improving model training.\n",
    "- Some categorical columns have substantial missing values:\n",
    "    - `Brand`: 9705 missing in `train`, 117,000 missing in `train_extra`\n",
    "    - `Material`, `Style`, `Color`\n",
    "- `train_extra` has a higher proportion of missing values.\n",
    "- Considering:\n",
    "    - Mode imputation for categorical columns\n",
    "    - Mean/median imputation for numerical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Distribution\n",
    "\n",
    "Let's take a look at the distribution of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier boxplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=train_df[\"Price\"])\n",
    "plt.title(\"Boxplot of Backpack Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature distribution\n",
    "for col in cat_columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.countplot(y=train_df[col], order=train_df[col].value_counts().index, hue=train_df[col], palette=\"coolwarm\", legend=False)\n",
    "    plt.title(f\"Count Plot of {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "In this section, we will preprocess the data to prepare it for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering\n",
    "\n",
    "In this section, we will create new features that may help improve the performance of our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of Combined (Combined_list) Features\n",
    "\n",
    "For each original categorical column, a new feature is generated by combining it with `Weight Capacity`.\n",
    "\n",
    "This is done to create a new feature that captures the interaction between the original categorical feature and the weight capacity of the backpack. The new feature is created by multiplying the weight capacity by 100 and adding it to the original categorical feature. This allows us to create a new feature that captures the interaction between the original categorical feature and the weight capacity of the backpack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = []\n",
    "label_encoders = {}\n",
    "\n",
    "for c in cat_columns:  # Use 'cat_columns' as defined earlier in the notebook\n",
    "    # Initialize and fit a LabelEncoder for the current column\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[c], test_df[c]], axis=0)\n",
    "    le.fit(combined)\n",
    "    label_encoders[c] = le  # Store the encoder for potential future use\n",
    "\n",
    "    # Transform the train and test data\n",
    "    train_df[c] = le.transform(train_df[c])\n",
    "    test_df[c] = le.transform(test_df[c])\n",
    "\n",
    "    # Create a new column combining the encoded value and weight capacity\n",
    "    new_col = f\"{c}_Weight_Capacity_Combined\"\n",
    "    train_df[new_col] = train_df[c] * 100 + train_df[\"Weight Capacity (kg)\"]\n",
    "    test_df[new_col] = test_df[c] * 100 + test_df[\"Weight Capacity (kg)\"]\n",
    "\n",
    "    # Append the new column name to the combined_list list\n",
    "    combined_list.append(new_col)\n",
    "\n",
    "print(f\"We now have {len(combined_list)} new columns\")\n",
    "print(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables_le = cat_columns + num_columns + combined_list\n",
    "print(f\"We now have {len(input_variables_le)} columns:\")\n",
    "print(input_variables_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = cat_columns + num_columns\n",
    "print(f\"We now have {len(input_variables)} columns:\")\n",
    "print(input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input features and target variable\n",
    "X_le = train_df[input_variables_le]\n",
    "y = train_df['Price']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train_le, X_temp_le, y_train_le, y_temp_le = train_test_split(X_le, y, test_size=0.3, random_state=42)\n",
    "X_valid_le, X_test_le, y_valid_le, y_test_le = train_test_split(X_temp_le, y_temp_le, test_size=0.5, random_state=42)\n",
    "\n",
    "# For the test dataset (test.csv), ensure it only contains the input features\n",
    "X_test_final = test_df[input_variables_le]\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(f\"X_train_le shape: {X_train_le.shape}\")\n",
    "print(f\"y_train shape: {y_train_le.shape}\")\n",
    "print(f\"X_valid shape: {X_valid_le.shape}\")\n",
    "print(f\"y_valid shape: {y_valid_le.shape}\")\n",
    "print(f\"X_test shape: {X_test_le.shape}\")\n",
    "print(f\"y_test shape: {y_test_le.shape}\")\n",
    "print(f\"X_test_final shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input features and target variable\n",
    "X = train_df[input_variables]\n",
    "y = train_df['Price']\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# For the test dataset (test.csv), ensure it only contains the input features\n",
    "X_test_final = test_df[input_variables]\n",
    "\n",
    "# Display the shapes of the datasets\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(f\"X_test_final shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "In this section, we will select and train machine learning models to predict the price of the backpack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into DMatrix format for XGBoost\n",
    "dtrain = xgb.DMatrix(X_train_le, label=y_train_le)\n",
    "dvalid = xgb.DMatrix(X_valid_le, label=y_valid_le)\n",
    "dtest = xgb.DMatrix(X_test_le)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "le_xgb_model = xgb.train(params, dtrain, num_boost_round=1_000, evals=evals, early_stopping_rounds=50, verbose_eval=15)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "le_y_pred_valid = le_xgb_model.predict(dvalid)\n",
    "\n",
    "# Make predictions on the test set\n",
    "le_y_pred_test = le_xgb_model.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# y_pred_lr = 0\n",
    "\n",
    "def evaluate_imputation_strategy(imputer, X_train, X_valid, y_train, y_valid):\n",
    "    \"\"\"\n",
    "    Fits imputer on X_train, transforms X_valid,\n",
    "    encodes categoricals, trains a linear regression,\n",
    "    and returns validation MAE and RMSE.\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # deep copy\n",
    "    X_train_copy = deepcopy(X_train)\n",
    "    X_valid_copy = deepcopy(X_valid)\n",
    "\n",
    "    # separate numeric and categorical columns\n",
    "    numeric_cols = X_train_copy.select_dtypes(include=[np.number]).columns\n",
    "    cat_cols = X_train_copy.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # 1. numeric imputation\n",
    "    X_train_copy[numeric_cols] = imputer.fit_transform(X_train_copy[numeric_cols])\n",
    "    X_valid_copy[numeric_cols] = imputer.transform(X_valid_copy[numeric_cols])\n",
    "\n",
    "    # 2. fill categorical missing values\n",
    "    X_train_copy[cat_cols] = X_train_copy[cat_cols].fillna(\"Missing\")\n",
    "    X_valid_copy[cat_cols] = X_valid_copy[cat_cols].fillna(\"Missing\")\n",
    "\n",
    "    # 3. one-hot encode categorical columns\n",
    "    X_train_copy = pd.get_dummies(X_train_copy, columns=cat_cols)\n",
    "    X_valid_copy = pd.get_dummies(X_valid_copy, columns=cat_cols)\n",
    "\n",
    "    # 4. align columns (to match dummy columns between sets)\n",
    "    X_train_copy, X_valid_copy = X_train_copy.align(X_valid_copy, join='left', axis=1)\n",
    "    X_valid_copy = X_valid_copy.fillna(0)\n",
    "\n",
    "    # 5. train and evaluate\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_copy, y_train)\n",
    "    y_pred = model.predict(X_valid_copy)\n",
    "    mae = mean_absolute_error(y_valid, y_pred)\n",
    "    rmse = root_mean_squared_error(y_valid, y_pred, squared=False)\n",
    "\n",
    "    return mae, rmse\n",
    "\n",
    "simple_median_imputer = SimpleImputer(strategy='median')\n",
    "mae_median, rmse_median = evaluate_imputation_strategy(simple_median_imputer, X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "mae_knn, rmse_knn = evaluate_imputation_strategy(knn_imputer, X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "print(\"MAE with Median Imputer:\", mae_median)\n",
    "print(\"RMSE with Median Imputer:\", rmse_median)\n",
    "print(\"MAE with KNN Imputer:\", mae_knn)\n",
    "print(\"RMSE with KNN Imputer:\", rmse_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values by filling them with the median\n",
    "X_train_le = X_train_le.fillna(X_train_le.median())\n",
    "X_valid_le = X_valid_le.fillna(X_valid_le.median())\n",
    "X_test_le = X_test_le.fillna(X_test_le.median())\n",
    "\n",
    "# Train the Linear Regression model\n",
    "le_lr_model = LinearRegression()\n",
    "le_lr_model.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "le_y_pred_lr = le_lr_model.predict(X_valid_le)\n",
    "\n",
    "# Make predictions on the test set\n",
    "le_y_pred_test_lr = le_lr_model.predict(X_test_le)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions on the test set:\", le_y_pred_test_lr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing cells in numeric columns\n",
    "missing_counts = X_train.select_dtypes(include=[np.number]).isna().sum()\n",
    "print(missing_counts[missing_counts > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They both have the same MAE and RMSE, so we will go ahead with the median approach, as it is simpler and faster at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric vs. categorical columns\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "cat_cols = X_train.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Fill numeric columns with X_train's median\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X_train[numeric_cols] = numeric_imputer.fit_transform(X_train[numeric_cols])\n",
    "X_valid[numeric_cols] = numeric_imputer.transform(X_valid[numeric_cols])\n",
    "X_test[numeric_cols] = numeric_imputer.transform(X_test[numeric_cols])\n",
    "\n",
    "# Fill categorical columns with \"Missing\"\n",
    "X_train[cat_cols] = X_train[cat_cols].fillna(\"Missing\")\n",
    "X_valid[cat_cols] = X_valid[cat_cols].fillna(\"Missing\")\n",
    "X_test[cat_cols] = X_test[cat_cols].fillna(\"Missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will standardize numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit on X_train numeric features\n",
    "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "X_valid[numeric_cols] = scaler.transform(X_valid[numeric_cols])\n",
    "X_test[numeric_cols]  = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# (Optional) If we use X_test_final for Kaggle submission\n",
    "#X_test_final[numeric_cols] = scaler.transform(X_test_final[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And perform one-hot encoding on the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# instantiate the encoder\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# fit on train's categorical columns\n",
    "ohe.fit(X_train[cat_cols])\n",
    "\n",
    "# transform each dataset's categorical columns\n",
    "X_train_ohe = ohe.transform(X_train[cat_cols])\n",
    "X_valid_ohe = ohe.transform(X_valid[cat_cols])\n",
    "X_test_ohe  = ohe.transform(X_test[cat_cols])\n",
    "\n",
    "# convert numeric columns to arrays\n",
    "X_train_numeric = X_train[numeric_cols].values\n",
    "X_valid_numeric = X_valid[numeric_cols].values\n",
    "X_test_numeric  = X_test[numeric_cols].values\n",
    "\n",
    "# concatenate numeric and encoded categorical data\n",
    "X_train_final = np.concatenate([X_train_numeric, X_train_ohe], axis=1)\n",
    "X_valid_final = np.concatenate([X_valid_numeric, X_valid_ohe], axis=1)\n",
    "X_test_final_2 = np.concatenate([X_test_numeric, X_test_ohe], axis=1)\n",
    "\n",
    "# reminder: if we want to make final Kaggle submissions on test_df,\n",
    "# we need to transform X_test_final the same way, if it exists.\n",
    "# e.g.\n",
    "# X_test_final_ohe = ohe.transform(X_test_final[cat_cols])\n",
    "# X_test_final_numeric = X_test_final[numeric_cols].values\n",
    "# X_test_final_new = np.concatenate([X_test_final_numeric, X_test_final_ohe], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT NOTE TO SELF:**\n",
    "Make the same changes for XGBoost as well, or any other model you train. For example:\n",
    "\n",
    "xgb_regressor.fit(X_train_final, y_train, eval_set=[(X_train_final, y_train), (X_valid_final, y_valid)], verbose=15)\n",
    "...\n",
    "y_pred_test_xgb = xgb_regressor.predict(X_test_final_2)\n",
    "\n",
    "Otherwise, we'll be mixing data that’s not one-hot-encoded for XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Linear Regression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_lr = lr_model.predict(X_valid_final)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_lr = lr_model.predict(X_test_final_2)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions on the test set:\", y_pred_test_lr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the one-hot encoded data into DMatrix format\n",
    "dtrain = xgb.DMatrix(X_train_final, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid_final, label=y_valid)\n",
    "dtest = xgb.DMatrix(X_test_final_2)\n",
    "\n",
    "# Define the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'learning_rate': 0.02,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "evals = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=15\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_valid = xgb_model.predict(dvalid)\n",
    "y_pred_test = xgb_model.predict(dtest)\n",
    "\n",
    "print(\"First few predictions on the test set:\", y_pred_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBRegressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBRegressor model\n",
    "xgb_regressor = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    early_stopping_rounds=50,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "xgb_regressor.fit(\n",
    "    X_train_final,\n",
    "    y_train,\n",
    "    eval_set=[(X_train_final, y_train), (X_valid_final, y_valid)],\n",
    "    verbose=15\n",
    ")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_valid_xgb = xgb_regressor.predict(X_valid_final)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_xgb = xgb_regressor.predict(X_test_final_2)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions on the test set:\", y_pred_test_xgb[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=30,\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='log2',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train_final, y_train)\n",
    "\n",
    "#predictions\n",
    "y_pred_valid_rf = rf_model.predict(X_valid_final)\n",
    "y_pred_test_rf  = rf_model.predict(X_test_final_2)\n",
    "\n",
    "# evaluate\n",
    "rf_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid_rf))\n",
    "print(f\"Random Forest Validation RMSE: {rf_rmse_valid:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase n estimators and depth\n",
    "rf_model2 = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=14,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    max_features='log2',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model2.fit(X_train_final, y_train)\n",
    "\n",
    "#predictions\n",
    "y_pred_valid_rf = rf_model2.predict(X_valid_final)\n",
    "y_pred_test_rf  = rf_model2.predict(X_test_final_2)\n",
    "\n",
    "# evaluate\n",
    "rf_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid_rf))\n",
    "print(f\"Random Forest Validation RMSE: {rf_rmse_valid:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marginal improvement in RMSE with more estimators and higher depth. There is no meaningful gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Ensure a GPU is available (Runtime ▸ Change runtime type ▸ GPU)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "INPUT_DIM = X_train_final.shape[1]\n",
    "\n",
    "# ── Build the network ─────────────────────────────────────────\n",
    "def make_model(input_dim):\n",
    "    return keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(input_dim,)),\n",
    "        keras.layers.Dense(256, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "\n",
    "        keras.layers.Dense(128, activation='relu'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Dropout(0.2),\n",
    "\n",
    "        keras.layers.Dense(1)   # linear output for regression\n",
    "    ])\n",
    "\n",
    "model = make_model(INPUT_DIM)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='mse',\n",
    "    metrics=[keras.metrics.RootMeanSquaredError(name='rmse')]\n",
    ")\n",
    "\n",
    "# ── Train ─────────────────────────────────────────────────────\n",
    "EPOCHS      = 20\n",
    "BATCH_SIZE  = 1024   # fits easily in GPU memory\n",
    "early_stop  = keras.callbacks.EarlyStopping(\n",
    "    patience=3, restore_best_weights=True, monitor='val_rmse'\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train,\n",
    "    validation_data=(X_valid_final, y_valid),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# ── Evaluate ──────────────────────────────────────────────────\n",
    "y_pred_nn = model.predict(X_valid_final, batch_size=4096).squeeze()\n",
    "nn_rmse   = np.sqrt(mean_squared_error(y_valid, y_pred_nn))\n",
    "print(f\"Neural‑Net Validation RMSE: {nn_rmse:.3f}\")\n",
    "\n",
    "# ── Predict test if desired ──────────────────────────────────\n",
    "y_pred_test_nn = model.predict(X_test_final_2, batch_size=4096).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the XGBRegressor model\n",
    "le_xgb_regressor = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    learning_rate=0.02,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    early_stopping_rounds=50, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "le_xgb_regressor.fit(\n",
    "    X_train_le, \n",
    "    y_train_le, \n",
    "    eval_set=[(X_train_le, y_train_le), (X_valid_le, y_valid_le)], \n",
    "    verbose=15\n",
    ")\n",
    "\n",
    "# Make predictions on the validation set\n",
    "le_y_pred_valid_xgb = le_xgb_regressor.predict(X_valid_le)\n",
    "\n",
    "# Make predictions on the test set\n",
    "le_y_pred_test_xgb = le_xgb_regressor.predict(X_test_le)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions on the test set:\", le_y_pred_test_xgb[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "In this section, we will evaluate the performance of our models using various metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the xgb model using RMSE\n",
    "xgb_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid))\n",
    "print(f\"Baseline - XGBoost Validation RMSE: {xgb_rmse_valid}\")\n",
    "\n",
    "# Calculate RMSE for the validation set\n",
    "xgbreg_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid_xgb))\n",
    "print(f\"Baseline - XGBoost Regressor Validation RMSE: {xgbreg_rmse_valid}\")\n",
    "\n",
    "# Evaluate the linear regression model using RMSE\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_valid, y_pred_lr))\n",
    "print(f\"Baseline - Linear Regression Validation RMSE: {rmse_lr}\")\n",
    "\n",
    "# Evaluate the random forest model using RMSE\n",
    "rf_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid_rf))\n",
    "print(f\"Baseline - Random Forest Validation RMSE: {rf_rmse_valid}\")\n",
    "\n",
    "# Evaluate the neural network model using RMSE\n",
    "nn_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_nn))\n",
    "print(f\"Baseline - Neural Network Validation RMSE: {nn_rmse_valid}\")\n",
    "\n",
    "# Evaluate the LE xgb model using RMSE\n",
    "le_xgb_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, le_y_pred_valid))\n",
    "print(f\"Baseline - LE XGBoost Validation RMSE: {le_xgb_rmse_valid}\")\n",
    "\n",
    "# Calculate RMSE for the LE xgb regressor validation set\n",
    "le_xgbreg_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, le_y_pred_valid_xgb))\n",
    "print(f\"Baseline - LE XGBoost Regressor Validation RMSE: {le_xgbreg_rmse_valid}\")\n",
    "\n",
    "# Evaluate the LE linear regression model using RMSE\n",
    "le_rmse_lr = np.sqrt(mean_squared_error(y_valid_le, le_y_pred_lr))\n",
    "print(f\"Baseline - LE Linear Regression Validation RMSE: {le_rmse_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# Log the XGBoost Booster model\n",
    "mlflow.xgboost.log_model(le_xgb_model, artifact_path=\"le_xgb_booster_model\", registered_model_name=\"le_xgb_booster_model\")\n",
    "\n",
    "# Log the XGBoost Regressor model\n",
    "mlflow.sklearn.log_model(le_xgb_regressor, artifact_path=\"le_xgb_regressor_model\", registered_model_name=\"le_xgb_regressor_model\")\n",
    "\n",
    "# Log the Linear Regression model (Label Encoded)\n",
    "mlflow.sklearn.log_model(le_lr_model, artifact_path=\"le_linear_regression_model\", registered_model_name=\"le_linear_regression_model\")\n",
    "\n",
    "# Log the XGBoost Booster model (One-Hot Encoded)\n",
    "mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgb_booster_model\", registered_model_name=\"xgb_booster_model\")\n",
    "\n",
    "# Log the XGBoost Regressor model (One-Hot Encoded)\n",
    "mlflow.sklearn.log_model(xgb_regressor, artifact_path=\"xgb_regressor_model\", registered_model_name=\"xgb_regressor_model\")\n",
    "\n",
    "# Log the Linear Regression model (One-Hot Encoded)\n",
    "mlflow.sklearn.log_model(lr_model, artifact_path=\"linear_regression_model\", registered_model_name=\"linear_regression_model\")\n",
    "\n",
    "# Log the Random Forest model\n",
    "mlflow.sklearn.log_model(rf_model, artifact_path=\"random_forest_model\", registered_model_name=\"random_forest_model\")\n",
    "\n",
    "# Log the Random Forest model with increased depth and estimators\n",
    "mlflow.sklearn.log_model(rf_model2, artifact_path=\"random_forest_model_v2\", registered_model_name=\"random_forest_model_v2\")\n",
    "\n",
    "# Log the Neural Network model\n",
    "mlflow.tensorflow.log_model(model, artifact_path=\"neural_network_model\", registered_model_name=\"neural_network_model\")\n",
    "\n",
    "print(\"All models logged successfully as MLflow models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the XGBoost Booster model locally\n",
    "# mlflow.sklearn.save_model(xgb_model, path='../models/baseline/xgb_booster_model')\n",
    "\n",
    "# # Save the XGBoost Regressor model locally\n",
    "# mlflow.sklearn.save_model(xgb_regressor, path='../models/baseline/xgb_rgr_model')\n",
    "\n",
    "# # Save the Linear Regression model locally using mlflow\n",
    "# mlflow.sklearn.save_model(lr_model, path='../models/baseline/lr_model')\n",
    "\n",
    "# print(\"Models saved locally.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Model Optimization\n",
    "\n",
    "In this section, we will optimize the hyperparameters of our models to improve their performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs hyperparameter tuning for an ElasticNet regression model using `GridSearchCV`. It defines a parameter grid for `alpha` and `l1_ratio`, trains the model on the training dataset (`X_train_le` and `y_train_le`), and evaluates it on the validation dataset (`X_valid_le` and `y_valid_le`). The best parameters and validation RMSE are logged using MLflow, and the trained model is saved. Finally, predictions are made on the test dataset (`X_test_le`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"ElasticNet Hyperparameter Tuning\"):\n",
    "    # Define the parameter grid for ElasticNet\n",
    "    elasticnet_param_grid = {\n",
    "        'alpha': [0.01, 0.05, 0.1, 0.5, 1.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "\n",
    "    # Initialize the ElasticNet model\n",
    "    elasticnet_model = ElasticNet()\n",
    "\n",
    "    # Perform GridSearchCV to find the best hyperparameters\n",
    "    grid_search_en = GridSearchCV(estimator=elasticnet_model, param_grid=elasticnet_param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
    "    grid_search_en.fit(X_train_le, y_train_le)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    elasticnet_model = grid_search_en.best_estimator_\n",
    "    best_params_en = grid_search_en.best_params_\n",
    "    print(f\"Best Parameters for ElasticNet: {best_params_en}\")\n",
    "\n",
    "    # Log the best parameters\n",
    "    mlflow.log_params(best_params_en)\n",
    "\n",
    "    # Train the model on the training dataset\n",
    "    elasticnet_model.fit(X_train_le, y_train_le)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_valid_en = elasticnet_model.predict(X_valid_le)\n",
    "\n",
    "    # Evaluate the model using RMSE\n",
    "    en_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, y_pred_valid_en))\n",
    "    print(f\"ElasticNet Validation RMSE: {en_rmse_valid}\")\n",
    "\n",
    "    # Log the RMSE metric\n",
    "    mlflow.log_metric(\"Validation RMSE\", en_rmse_valid)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_test_en = elasticnet_model.predict(X_test_le)\n",
    "\n",
    "    # Log the ElasticNet model\n",
    "    mlflow.sklearn.log_model(elasticnet_model, artifact_path=\"le_elasticnet_model\", registered_model_name=\"le_elasticnet_model\")\n",
    "\n",
    "    # Display the first few predictions\n",
    "    print(\"First few predictions on the test set:\", y_pred_test_en[:5])\n",
    "\n",
    "print(\"ElasticNet model training and tracking completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs hyperparameter tuning for a `GradientBoostingRegressor` model using `GridSearchCV`. Here's a summary of what it does:\n",
    "\n",
    "1. **Define Parameter Grid**: It specifies a grid of hyperparameters for the `GradientBoostingRegressor`, including `learning_rate`, `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `subsample`, `max_features`, `loss`, and `criterion`.\n",
    "\n",
    "2. **Initialize the Model**: It initializes a `GradientBoostingRegressor` instance.\n",
    "\n",
    "3. **Grid Search with Cross-Validation**: It uses `GridSearchCV` to search for the best combination of hyperparameters based on the negative mean squared error metric. The search is performed using 3-fold cross-validation.\n",
    "\n",
    "4. **Train the Model**: It fits the model on the training dataset (`X_train` and `y_train`).\n",
    "\n",
    "5. **Log Best Parameters**: It logs the best hyperparameters found during the grid search using MLflow.\n",
    "\n",
    "6. **Evaluate the Model**: It makes predictions on the validation dataset (`X_valid`) and calculates the RMSE to evaluate the model's performance.\n",
    "\n",
    "7. **Log the Model**: It logs the trained `GradientBoostingRegressor` model to MLflow for tracking and reproducibility.\n",
    "\n",
    "8. **Make Predictions**: It makes predictions on the test dataset (`X_test`) for further analysis.\n",
    "\n",
    "This cell is part of the model optimization process to improve the performance of the `GradientBoostingRegressor` by finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for GradientBoostingRegressor\n",
    "param_grid = {\n",
    "    'learning_rate': [0.04, 0.045],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [6],\n",
    "    'min_samples_split': [0.00075],\n",
    "    'min_samples_leaf': [0.0074],\n",
    "    'subsample': [0.6],\n",
    "    'max_features': [0.15],\n",
    "    'loss': ['huber'],\n",
    "    'criterion': ['squared_error']\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"GradientBoostingRegressor Hyperparameter Tuning\"):\n",
    "    # Perform GridSearchCV to find the best hyperparameters\n",
    "    grid_search_gbr = GridSearchCV(estimator=gbr, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
    "    grid_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best model and parameters\n",
    "    gbr_model = grid_search_gbr.best_estimator_\n",
    "    best_params_gbr = grid_search_gbr.best_params_\n",
    "    print(f\"Best Parameters for GradientBoostingRegressor: {best_params_gbr}\")\n",
    "\n",
    "    # Log the best parameters\n",
    "    mlflow.log_params(best_params_gbr)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_valid_gbr = gbr_model.predict(X_valid)\n",
    "\n",
    "    # Evaluate the model using RMSE\n",
    "    gbr_rmse_valid = np.sqrt(mean_squared_error(y_valid, y_pred_valid_gbr))\n",
    "    print(f\"GradientBoostingRegressor Validation RMSE: {gbr_rmse_valid}\")\n",
    "\n",
    "    # Log the RMSE metric\n",
    "    mlflow.log_metric(\"Validation RMSE\", gbr_rmse_valid)\n",
    "\n",
    "    # Log the GradientBoostingRegressor model\n",
    "    mlflow.sklearn.log_model(gbr_model, artifact_path=\"gradient_boosting_model\", registered_model_name=\"gradient_boosting_model\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_test_gbr = gbr_model.predict(X_test)\n",
    "\n",
    "    # Save the predictions for further analysis\n",
    "    # test_predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test_gbr})\n",
    "    # test_predictions.to_csv(\"../data/processed/gbr_test_predictions.csv\", index=False)\n",
    "\n",
    "    # Log the predictions file\n",
    "    # mlflow.log_artifact(\"../data/processed/gbr_test_predictions.csv\")\n",
    "\n",
    "print(\"GradientBoostingRegressor model training and tracking completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs hyperparameter tuning for a LightGBM model using `GridSearchCV`. Here's what it does:\n",
    "\n",
    "1. **Define Parameter Grid**: Specifies a grid of hyperparameters for the LightGBM model, including `num_leaves`, `max_depth`, `learning_rate`, `n_estimators`, `subsample`, and `colsample_bytree`.\n",
    "\n",
    "2. **Initialize the Model**: Creates an instance of the `LGBMRegressor` model with a fixed random state for reproducibility.\n",
    "\n",
    "3. **Grid Search with Cross-Validation**: Uses `GridSearchCV` to search for the best combination of hyperparameters based on the negative mean squared error metric. The search is performed using 3-fold cross-validation.\n",
    "\n",
    "4. **Train the Model**: Fits the model on the training dataset (`X_train_le` and `y_train_le`).\n",
    "\n",
    "5. **Log Best Parameters**: Logs the best hyperparameters found during the grid search using MLflow.\n",
    "\n",
    "6. **Evaluate the Model**: Makes predictions on the validation dataset (`X_valid_le`) and calculates the RMSE to evaluate the model's performance.\n",
    "\n",
    "7. **Log the Model**: Logs the trained LightGBM model to MLflow for tracking and reproducibility.\n",
    "\n",
    "8. **Make Predictions**: Makes predictions on the test dataset (`X_test_le`) for further analysis.\n",
    "\n",
    "This cell is part of the model optimization process to improve the performance of the LightGBM model by finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import mlflow.lightgbm\n",
    "\n",
    "# Define the parameter grid for LightGBM\n",
    "param_grid = {\n",
    "    'num_leaves': [3],\n",
    "    'max_depth': [5, 10],\n",
    "    'learning_rate': [0.08],\n",
    "    'n_estimators': [200],\n",
    "    'subsample': [0.65],\n",
    "    'colsample_bytree': [1.0]\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_lgb_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"LightGBM Hyperparameter Tuning\"):\n",
    "    # Log the best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_valid_lgb = best_lgb_model.predict(X_valid_le)\n",
    "\n",
    "    # Evaluate the model using RMSE\n",
    "    lgb_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, y_pred_valid_lgb))\n",
    "    print(f\"LightGBM Validation RMSE: {lgb_rmse_valid}\")\n",
    "\n",
    "    # Log the RMSE metric\n",
    "    mlflow.log_metric(\"Validation RMSE\", lgb_rmse_valid)\n",
    "\n",
    "    # Log the LightGBM model\n",
    "    mlflow.lightgbm.log_model(best_lgb_model, artifact_path=\"le_lightgbm_model\", registered_model_name=\"le_lightgbm_model\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_test_lgb = best_lgb_model.predict(X_test_le)\n",
    "\n",
    "    # # Save the predictions for further analysis\n",
    "    # test_predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test_lgb})\n",
    "    # test_predictions.to_csv(\"../data/processed/lgb_test_predictions.csv\", index=False)\n",
    "\n",
    "    # # Log the predictions file\n",
    "    # mlflow.log_artifact(\"../data/processed/lgb_test_predictions.csv\")\n",
    "\n",
    "print(\"LightGBM model training and tracking completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs hyperparameter tuning for a LightGBM model using `GridSearchCV`. Here's what it does:\n",
    "\n",
    "1. **Define Parameter Grid**: Specifies a grid of hyperparameters for the LightGBM model, including `num_leaves`, `max_depth`, `learning_rate`, `n_estimators`, `subsample`, and `colsample_bytree`.\n",
    "\n",
    "2. **Initialize the Model**: Creates an instance of the `LGBMRegressor` model with a fixed random state for reproducibility.\n",
    "\n",
    "3. **Grid Search with Cross-Validation**: Uses `GridSearchCV` to search for the best combination of hyperparameters based on the negative mean squared error metric. The search is performed using 3-fold cross-validation.\n",
    "\n",
    "4. **Train the Model**: Fits the model on the training dataset (`X_train_le` and `y_train_le`).\n",
    "\n",
    "5. **Log Best Parameters**: Logs the best hyperparameters found during the grid search using MLflow.\n",
    "\n",
    "6. **Evaluate the Model**: Makes predictions on the validation dataset (`X_valid_le`) and calculates the RMSE to evaluate the model's performance.\n",
    "\n",
    "7. **Log the Model**: Logs the trained LightGBM model to MLflow for tracking and reproducibility.\n",
    "\n",
    "8. **Make Predictions**: Makes predictions on the test dataset (`X_test_le`) for further analysis.\n",
    "\n",
    "This cell is part of the model optimization process to improve the performance of the LightGBM model by finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import mlflow.lightgbm\n",
    "\n",
    "# Define the parameter grid for LightGBM\n",
    "param_grid = {\n",
    "    'num_leaves': [3],\n",
    "    'max_depth': [5, 10],\n",
    "    'learning_rate': [0.08],\n",
    "    'n_estimators': [200],\n",
    "    'subsample': [0.65],\n",
    "    'colsample_bytree': [1.0]\n",
    "}\n",
    "\n",
    "# Initialize the LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_lgb_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"LightGBM Hyperparameter Tuning\"):\n",
    "    # Log the best parameters\n",
    "    mlflow.log_params(best_params)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred_valid_lgb = best_lgb_model.predict(X_valid_le)\n",
    "\n",
    "    # Evaluate the model using RMSE\n",
    "    lgb_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, y_pred_valid_lgb))\n",
    "    print(f\"LightGBM Validation RMSE: {lgb_rmse_valid}\")\n",
    "\n",
    "    # Log the RMSE metric\n",
    "    mlflow.log_metric(\"Validation RMSE\", lgb_rmse_valid)\n",
    "\n",
    "    # Log the LightGBM model\n",
    "    mlflow.lightgbm.log_model(best_lgb_model, artifact_path=\"le_lightgbm_model\", registered_model_name=\"le_lightgbm_model\")\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred_test_lgb = best_lgb_model.predict(X_test)\n",
    "\n",
    "    # # Save the predictions for further analysis\n",
    "    # test_predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test_lgb})\n",
    "    # test_predictions.to_csv(\"../data/processed/lgb_test_predictions.csv\", index=False)\n",
    "\n",
    "    # # Log the predictions file\n",
    "    # mlflow.log_artifact(\"../data/processed/lgb_test_predictions.csv\")\n",
    "\n",
    "print(\"LightGBM model training and tracking completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell performs hyperparameter tuning for a `GradientBoostingRegressor` model using `GridSearchCV`. Here's what it does:\n",
    "\n",
    "1. **Define Parameter Grid**: Specifies a grid of hyperparameters for the `GradientBoostingRegressor`, including `learning_rate`, `n_estimators`, `max_depth`, `min_samples_split`, `min_samples_leaf`, `subsample`, `max_features`, `loss`, and `criterion`.\n",
    "\n",
    "2. **Initialize GridSearchCV**: Sets up `GridSearchCV` to search for the best combination of hyperparameters using the negative root mean squared log error (`neg_root_mean_squared_log_error`) as the scoring metric. It uses 3-fold cross-validation.\n",
    "\n",
    "3. **Fit the Model**: Trains the `GradientBoostingRegressor` on the training dataset (`X_train_le` and `y_train_le`) using the specified parameter grid.\n",
    "\n",
    "4. **Retrieve Best Parameters**: Extracts the best hyperparameters and the corresponding model from the grid search.\n",
    "\n",
    "5. **Train the Best Model**: Fits the best model on the training dataset.\n",
    "\n",
    "6. **Evaluate the Model**: Makes predictions on the validation dataset (`X_valid_le`) and calculates the RMSE to evaluate the model's performance.\n",
    "\n",
    "7. **Log with MLflow**: Logs the model's parameters, validation RMSE, and the trained model itself to MLflow for tracking and reproducibility.\n",
    "\n",
    "This cell is part of the model optimization process to improve the performance of the `GradientBoostingRegressor` by finding the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Define the parameter grid for GradientBoostingRegressor\n",
    "param_grid = {\n",
    "    'learning_rate': [0.04, 0.045],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [6],\n",
    "    'min_samples_split': [0.00075],\n",
    "    'min_samples_leaf': [0.0074],\n",
    "    'subsample': [0.6],\n",
    "    'max_features': [0.1],\n",
    "    'loss': ['huber'],\n",
    "    'criterion': ['squared_error']\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search_gbr = GridSearchCV(\n",
    "    estimator=GradientBoostingRegressor(),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_root_mean_squared_log_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV on the training data\n",
    "grid_search_gbr.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Get the best model and parameters\n",
    "gbr_model = grid_search_gbr.best_estimator_\n",
    "best_params_gbr = grid_search_gbr.best_params_\n",
    "print(f\"Best Parameters for GradientBoostingRegressor: {best_params_gbr}\")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "gbr_model.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_valid_gbr = gbr_model.predict(X_valid_le)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "gbr_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, y_pred_valid_gbr))\n",
    "print(f\"GradientBoostingRegressor Validation RMSE: {gbr_rmse_valid}\")\n",
    "\n",
    "# Log the model and metrics with MLflow\n",
    "mlflow.start_run()\n",
    "mlflow.log_params(gbr_model.get_params())\n",
    "mlflow.log_metric(\"Validation RMSE\", gbr_rmse_valid)\n",
    "mlflow.sklearn.log_model(gbr_model, artifact_path=\"le_gradient_boosting_model\")\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"Gradient Boosting Regressor model training and tracking completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for ExtraTreesRegressor\n",
    "param_grid_et = {\n",
    "    'n_estimators': [10, 15],\n",
    "    'max_features': [0.6, 0.8],\n",
    "    'min_samples_split': [0.005],\n",
    "    'min_samples_leaf': [0.005],\n",
    "    'bootstrap': [False]\n",
    "}\n",
    "\n",
    "# Initialize the ExtraTreesRegressor\n",
    "extra_trees_model = ExtraTreesRegressor()\n",
    "\n",
    "# Perform GridSearchCV to find the best hyperparameters\n",
    "grid_search_et = GridSearchCV(\n",
    "    estimator=extra_trees_model,\n",
    "    param_grid=param_grid_et,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the GridSearchCV on the training data\n",
    "grid_search_et.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Get the best model and parameters\n",
    "extra_trees_model = grid_search_et.best_estimator_\n",
    "best_params_et = grid_search_et.best_params_\n",
    "print(f\"Best Parameters for ExtraTreesRegressor: {best_params_et}\")\n",
    "\n",
    "# Train the model on the training dataset\n",
    "extra_trees_model.fit(X_train_le, y_train_le)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_valid_et = extra_trees_model.predict(X_valid_le)\n",
    "\n",
    "# Evaluate the model using RMSE\n",
    "et_rmse_valid = np.sqrt(mean_squared_error(y_valid_le, y_pred_valid_et))\n",
    "print(f\"ExtraTreesRegressor Validation RMSE: {et_rmse_valid}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_et = extra_trees_model.predict(X_test)\n",
    "\n",
    "# Display the first few predictions\n",
    "print(\"First few predictions on the test set:\", y_pred_test_et[:5])\n",
    "\n",
    "\n",
    "# Log the model and metrics with MLflow\n",
    "mlflow.start_run(run_name=\"ExtraTreesRegressor Hyperparameter Tuning\")\n",
    "mlflow.log_params(best_params_et)\n",
    "mlflow.log_metric(\"Validation RMSE\", et_rmse_valid)\n",
    "mlflow.sklearn.log_model(extra_trees_model, artifact_path=\"le_extra_trees_model\", registered_model_name=\"le_extra_trees_model\")\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"ExtraTreesRegressor model training and tracking completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Final Submission\n",
    "\n",
    "In this section, we will select the best model and make final predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model(models, validation_predictions, y_valid, y_valid_le=None):\n",
    "    \"\"\"\n",
    "    Evaluates models based on RMSE and returns the best model.\n",
    "\n",
    "    Parameters:\n",
    "    - models (dict): A dictionary where keys are model names and values are model objects.\n",
    "    - validation_predictions (dict): A dictionary where keys are model names and values are predictions on the validation set.\n",
    "    - y_valid (pd.Series): The actual target values for the validation set (one-hot encoded).\n",
    "    - y_valid_le (pd.Series, optional): The actual target values for the validation set (label encoded).\n",
    "\n",
    "    Returns:\n",
    "    - best_model_name (str): The name of the best model.\n",
    "    - best_model (object): The best model object.\n",
    "    - best_rmse (float): The RMSE of the best model.\n",
    "    \"\"\"\n",
    "    best_model_name = None\n",
    "    best_model = None\n",
    "    best_rmse = float('inf')\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        predictions = validation_predictions[model_name]\n",
    "        target = y_valid_le if \"LE\" in model_name else y_valid\n",
    "        rmse = np.sqrt(mean_squared_error(target, predictions))\n",
    "        print(f\"{model_name} Validation RMSE: {rmse}\")\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model_name = model_name\n",
    "            best_model = model\n",
    "\n",
    "    print(f\"\\nBest Model: {best_model_name} with RMSE: {best_rmse}\")\n",
    "    return best_model_name, best_model, best_rmse\n",
    "\n",
    "# Example usage:\n",
    "models = {\n",
    "    \"LE XGBoost Booster\": le_xgb_model,\n",
    "    \"LE XGBoost Regressor\": le_xgb_regressor,\n",
    "    \"LE Linear Regression\": le_lr_model,\n",
    "    \"LE ElasticNet\": elasticnet_model,\n",
    "    \"LE Gradient Boosting Regressor\": gbr_model,\n",
    "    \"LE LightGBM\": best_lgb_model,\n",
    "    \"LE Extra Trees Regressor\": extra_trees_model,\n",
    "    \"XGBoost Booster\": xgb_model,\n",
    "    \"XGBoost Regressor\": xgb_regressor,\n",
    "    \"Linear Regression\": lr_model,\n",
    "    \"Random Forest Regressor\": rf_model,\n",
    "    \"Random Forest Regressor v2\": rf_model2,\n",
    "    \"Neural Network\": model\n",
    "}\n",
    "validation_predictions = {\n",
    "    \"LE XGBoost Booster\": le_y_pred_valid,\n",
    "    \"LE XGBoost Regressor\": le_y_pred_valid_xgb,\n",
    "    \"LE Linear Regression\": le_y_pred_lr,\n",
    "    \"LE ElasticNet\": y_pred_valid_en,\n",
    "    \"LE Gradient Boosting Regressor\": y_pred_valid_gbr,\n",
    "    \"LE LightGBM\": y_pred_valid_lgb,\n",
    "    \"LE Extra Trees Regressor\": y_pred_valid_et,\n",
    "    \"XGBoost Booster\": y_pred_valid,\n",
    "    \"XGBoost Regressor\": y_pred_valid_xgb,\n",
    "    \"Linear Regression\": y_pred_lr,\n",
    "    \"Random Forest Regressor\": y_pred_valid_rf,\n",
    "    \"Random Forest Regressor v2\": y_pred_valid_rf,\n",
    "    \"Neural Network\": y_pred_nn\n",
    "}\n",
    "\n",
    "best_model_name, best_model, best_rmse = get_best_model(models, validation_predictions, y_valid, y_valid_le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "\n",
    "def save_best_model(best_model, best_model_name, save_path=\"../models/best_model\"):\n",
    "    \"\"\"\n",
    "    Saves the best model and logs it with MLflow.\n",
    "\n",
    "    Parameters:\n",
    "    - best_model (object): The best model object to be saved.\n",
    "    - best_model_name (str): The name of the best model.\n",
    "    - save_path (str): The path where the model will be saved locally.\n",
    "    \"\"\"\n",
    "    # Log the best model with MLflow\n",
    "    mlflow.sklearn.log_model(best_model, artifact_path=save_path, registered_model_name=best_model_name)\n",
    "    print(f\"Best model '{best_model_name}' logged successfully with MLflow.\")\n",
    "\n",
    "    # Save the best model locally\n",
    "    mlflow.sklearn.save_model(best_model, path=save_path)\n",
    "    print(f\"Best model '{best_model_name}' saved locally at {save_path}.\")\n",
    "\n",
    "save_best_model(best_model=best_model, best_model_name=best_model_name)\n",
    "print(f\"Saved the best model locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "In this section, we will summarize our findings and discuss the implications of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Shuri Baseline Edits\n",
    "\n",
    "**DONE**:\n",
    "- one hot encoding categorcal\n",
    "- median impute for numerical and missing for categorical\n",
    "  - compared knnimpute vs median impute for numerical (same MAE on a simple linear model)\n",
    "- verify/removed \"multiplying weight capacity by 100\" feature\n",
    "- Center = 0 in Heatmap to highlight negative vs. positive correlation more clearly.\n",
    "- check missing values meaning -> don't have domain-specific knowledge (confirm this?)\n",
    "- standardized numeric features (inserted standardscaler step after imputing numeric cols, before ohe)\n",
    "\n",
    "note: i moved your xgboost after the OHE for cohesion + updated the variable names accordingly\n",
    "\n",
    "**TO DO**:\n",
    "\n",
    "- Do Hyperparameter Tuning: - thought you could edit your code for this @perry\n",
    "  - Instead of a single set of XGBoost parameters, do a small grid search or RandomizedSearchCV on learning_rate, max_depth, n_estimators, etc. Evaluate on validation set.\n",
    "- Try even more models (Neural Networks, Random Forest)\n",
    "- create final comparison table\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
