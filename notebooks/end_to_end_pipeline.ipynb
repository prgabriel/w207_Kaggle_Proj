{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpack Kaggle Competition\n",
    "### W207 Final Project - Spring 2025\n",
    "\n",
    "Team: Perry Gabriel, Aurelia Yang\n",
    "\n",
    "University of California, Berkeley\n",
    "\n",
    "## Description\n",
    "\n",
    "In this competition, participants are challenged to develop machine learning models to predict the price of a backpack based on various features. This is a great opportunity to test your skills, learn new techniques, and compete with others in the data science community.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Submissions are evaluated on the root mean squared error between the predicted and actual price of the backpack.\n",
    "\n",
    "RMSE is defined as:\n",
    "$$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "where $y_i$ is the actual price of the backpack and $\\hat{y}_i$ is the predicted price of the backpack.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data consists of the following columns:\n",
    "\n",
    "- `id`: A unique identifier for the backpack.\n",
    "- `Brand`: The brand of the backpack.\n",
    "- `Material`: The material of the backpack.\n",
    "- `Size`: The size of the backpack.\n",
    "- `Compartments`: The number of compartments in the backpack.\n",
    "- `Laptop Compartment`: Whether the backpack has a laptop compartment.\n",
    "- `Waterproof`: Whether the backpack is waterproof.\n",
    "- `Style`: The style of the backpack.\n",
    "- `Color`: The color of the backpack.\n",
    "- `Weight Capacity (kg)`: The weight capacity of the backpack in kilograms.\n",
    "- `Price`: The price of the backpack.\n",
    "\n",
    "## Data Splits\n",
    "The dataset is split into three parts:\n",
    "- **Train**: The training set contains 80% of the data and is used to train the model.\n",
    "- **Validation**: The validation set contains 10% of the data and is used to tune the model.\n",
    "- **Test**: The test set contains 10% of the data and is used to evaluate the model's performance.\n",
    "\n",
    "## Important Notes about the Dataset\n",
    "- There are (4) different datasets: train, train_extra, test, and sample_submission.\n",
    "- The `train` dataset contains the training data with the target variable `Price`.\n",
    "- The `train_extra` dataset contains additional training data that can be used to improve the model's performance.\n",
    "- The `test` dataset contains the test data without the target variable `Price`.\n",
    "- The `sample_submission` dataset contains a sample submission file with the correct format.\n",
    "- The `train` and `train_extra` datasets are combined to create a larger training set.\n",
    "- The `train_extra` dataset was provided by the competition organizers and is not part of the original dataset.\n",
    "\n",
    "## Submission File\n",
    "\n",
    "For each `id` in the test set, you must predict the price of the backpack. The file should contain a header and have the following format:\n",
    "\n",
    "```python\n",
    "id,Price\n",
    "1,100\n",
    "2,200\n",
    "3,300\n",
    "```\n",
    "\n",
    "## Timeline\n",
    "\n",
    "- **Start Date** - February 1, 2025\n",
    "- **Entry Deadline** - Same as the Final Submission Deadline\n",
    "- **Team Merger Deadline** - Same as the Final Submission Deadline\n",
    "- **Final Submission Deadline** - February 28, 2025\n",
    "\n",
    "All deadlines are at 11:59 PM UTC on the corresponding day unless otherwise noted. The competition organizers reserve the right to update the contest timeline if they deem it necessary.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "This dataset was created by [Kaggle](https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset) for the purpose of hosting a competition.\n",
    "\n",
    "## Team Members\n",
    "\n",
    "- [Perry Gabriel](https://www.kaggle.com/prgabriel)\n",
    "- [Aurelia Yang](https://www.kaggle.com/aureliayang)\n",
    "\n",
    "## Sections\n",
    "\n",
    "1. [Exploratory Data Analysis](#1.-Exploratory-Data-Analysis)\n",
    "2. [Data Preprocessing](#2.-Data-Preprocessing)\n",
    "3. [Modeling](#3.-Modeling)\n",
    "4. [Evaluation](#4.-Evaluation)\n",
    "5. [Optimization](#5.-Optimization)\n",
    "6. [Final Submission](#6.-Final-Submission)\n",
    "7. [Conclusion](#7.-Conclusion)\n",
    "\n",
    "## References\n",
    "[Backpack Kaggle Competition Link](https://www.kaggle.com/competitions/playground-series-s5e2)\n",
    "\n",
    "[Backpack Kaggle Competition Dataset](https://www.kaggle.com/datasets/souradippal/student-bag-price-prediction-dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    "Install the required libraries\n",
    "Uncomment to download the data from Kaggle. This assumes you have the Kaggle API installed and configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c playground-series-s5e2\n",
    "# !unzip playground-series-s5e2 -d ../data/raw/\n",
    "# !pip install -r ../requirements.txt\n",
    "# !rm -rf playground-series-s5e2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from category_encoders import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) #used to supress the tf version warning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name='E2E_Kaggle_Backpack_Project')\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path = '../data/raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google.colab\n",
    "\n",
    "def check_and_import_colab():\n",
    "    global raw_data_path\n",
    "    global processed_path\n",
    "    try:\n",
    "        from google.colab import drive  \n",
    "        import google.colab\n",
    "        print(\"Running on Google Colab\")\n",
    "        # Import necessary libraries for Google Colab\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "        # define paths\n",
    "        raw_data_path = \"/content/drive/MyDrive/Kaggle_Backpack/data/raw/\"\n",
    "        processed_path = \"/content/drive/MyDrive/Kaggle_Backpack/data/processed/\"\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Not running on Google Colab\")\n",
    "        os.makedirs(raw_data_path, exist_ok=True)\n",
    "        print(\"Created 'raw_data_path' directory for non-Colab Environment.\")\n",
    "        return False\n",
    "\n",
    "on_colab = check_and_import_colab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "In this section, we will explore the data to understand its structure and identify any patterns or trends that may be present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the Data\n",
    "\n",
    "Let's start by loading the data and taking a look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(filepath_or_buffer=os.path.join(raw_data_path, 'train.csv'), index_col=0, header=0, sep=',')\n",
    "test_df = pd.read_csv(filepath_or_buffer=os.path.join(raw_data_path, 'test.csv'), index_col=0, header=0, sep=',')\n",
    "train_extra_df = pd.read_csv(filepath_or_buffer=os.path.join(raw_data_path, 'training_extra.csv'), index_col=0, header=0, sep=',')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Summary\n",
    "\n",
    "Next, let's take a look at the summary statistics of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the summary statistics of the training data\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data types of columns in training dataset\\n{train_df.dtypes}\\n\")\n",
    "print(f\"Data types of columns in training extra dataset\\n{train_extra_df.dtypes}\\n\")\n",
    "print(f\"Data types of columns in testing dataset\\n{test_df.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the shape of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset.\n",
    "print(f\"Shape of training data: {train_df.shape}\")\n",
    "print(f\"Shape of training extra data: {train_extra_df.shape}\")\n",
    "print(f\"Shape of testing data: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's combine the train and train_extra datasets to create a larger training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and train_extra datasets\n",
    "train_df = pd.concat([train_df, train_extra_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Display the shape of the combined dataset\n",
    "print(f\"Shape of combined training data: {train_df.shape}\")\n",
    "\n",
    "# Display the shape of the combined dataset\n",
    "train_df.shape\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's capture the categories of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = train_df.columns[:-2].tolist()  # Dropped the last two columns since we know that these are numerical columns\n",
    "print(f'There are {len(cat_columns)} categorical columns:')\n",
    "print(cat_columns)\n",
    "\n",
    "num_columns = [train_df.columns[-2]]\n",
    "print(f'There are {len(num_columns)} numerical column:')\n",
    "print(num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Visualization\n",
    "\n",
    "We created visualizations to better understand the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, plot a histogram of the price column\n",
    "plt.hist(train_df['Price'], bins=20, edgecolor='black', color='skyblue', rwidth=0.8)\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Price (train_df)')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Correlation Matrix\n",
    "\n",
    "Finally, let's create a correlation matrix to see how the features are related to each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the numeric columns\n",
    "numeric_cols = train_df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr = numeric_cols.corr()\n",
    "\n",
    "# Display the correlation matrix\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations so far: \n",
    "\n",
    "- `training_extra` has significantly more records (3.69M) than `training` (300k), which will be useful in improving model training.\n",
    "- Some categorical columns have substantial missing values:\n",
    "    - `Brand`: 9705 missing in `train`, 117,000 missing in `train_extra`\n",
    "    - `Material`, `Style`, `Color`\n",
    "- `train_extra` has a higher proportion of missing values.\n",
    "- Considering:\n",
    "    - Mode imputation for categorical columns\n",
    "    - Mean/median imputation for numerical columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Feature Distribution\n",
    "\n",
    "Let's take a look at the distribution of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define numerical and categorical cols\n",
    "# num_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "# cat_cols = train_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# # remove price\n",
    "# num_cols.remove('Price')\n",
    "\n",
    "# # feature distribution comparison train.csv vs train_extra.csv\n",
    "# plt.figure(figsize=(12, 5))\n",
    "# sns.kdeplot(train_df['Price'], label='Train', fill=True)\n",
    "# sns.kdeplot(train_extra_df['Price'], label='Train Extra', fill=True)\n",
    "# plt.legend()\n",
    "# plt.title(\"Price Distribution Comparison (Train vs Train Extra)\")\n",
    "# plt.xlabel(\"Price\")\n",
    "# plt.ylabel(\"Density\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier boxplots\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=train_df[\"Price\"])\n",
    "plt.title(\"Boxplot of Backpack Prices\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical feature distribution\n",
    "for col in cat_columns:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    sns.countplot(y=train_df[col], order=train_df[col].value_counts().index, hue=train_df[col], palette=\"coolwarm\", legend=False)\n",
    "    plt.title(f\"Count Plot of {col}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations: Data Loading and Preprocessing\n",
    "\n",
    "#### Outlier Analysis\n",
    "- Slightly right-skewed distribution.\n",
    "- Consider removing outliers?\n",
    "\n",
    "#### Correlation Matrix\n",
    "- Weight capacity is slightly positively correlated with price (0.2).\n",
    "- Categorical feature impact on price needs boxplots by category -> build this out.\n",
    "\n",
    "#### Categorical Feature Distribution\n",
    "- No glaring abnormalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "In this section, we will preprocess the data to prepare it for modeling.\n",
    "\n",
    "### 2.1 Missing Values\n",
    "\n",
    "First, let's check for missing values in the data and decide how to handle them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_extra_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill in missing values categorical values with mode and numerical values with forward fill for both training datasets and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine train and train_extra\n",
    "# train_combined = pd.concat([train_df, train_extra_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "# # fill categorical with mode, others forward fill\n",
    "# for col in cat_cols:\n",
    "#     train_combined[col] = train_combined[col].fillna(train_combined[col].mode()[0])\n",
    "# test_df = test_df.ffill()\n",
    "\n",
    "# for col in num_cols:\n",
    "#     train_combined[col] = train_combined[col].fillna(train_combined[col].median())\n",
    "# test_df[num_cols] = test_df[num_cols].fillna(test_df[num_cols].median())\n",
    "\n",
    "# # one-hot encode (nominal categorical features)\n",
    "# ohe_cols = ['Brand', 'Color', 'Style']\n",
    "# train_encoded_df = pd.get_dummies(train_combined, columns=ohe_cols, drop_first=True)\n",
    "# test_encoded_df = pd.get_dummies(test_df, columns=ohe_cols, drop_first=True)\n",
    "\n",
    "# # ensure test set has same features as train\n",
    "# train_cols = train_encoded_df.columns\n",
    "# test_encoded_df = test_encoded_df.reindex(columns=train_cols, fill_value=0)\n",
    "\n",
    "# # ordinal feature label encoding\n",
    "# le_cols = ['Material', 'Size']\n",
    "# le = LabelEncoder()\n",
    "# for col in le_cols:\n",
    "#     train_encoded_df[col] = le.fit_transform(train_combined[col])\n",
    "#     test_encoded_df[col] = le.transform(test_df[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Feature Engineering\n",
    "\n",
    "In this section, we will create new features that may help improve the performance of our models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of Combined (Combined_list) Features\n",
    "\n",
    "For each original categorical column, a new feature is generated by combining it with Weight Capacity.\n",
    "\n",
    "This is done to create a new feature that captures the interaction between the original categorical feature and the weight capacity of the backpack. The new feature is created by multiplying the weight capacity by 100 and adding it to the original categorical feature. This allows us to create a new feature that captures the interaction between the original categorical feature and the weight capacity of the backpack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = []\n",
    "label_encoders = {}\n",
    "\n",
    "for c in cat_columns:  # Use 'cat_columns' as defined earlier in the notebook\n",
    "    # Initialize and fit a LabelEncoder for the current column\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[c], test_df[c]], axis=0)\n",
    "    le.fit(combined)\n",
    "    label_encoders[c] = le  # Store the encoder for potential future use\n",
    "\n",
    "    # Transform the train and test data\n",
    "    train_df[c] = le.transform(train_df[c])\n",
    "    test_df[c] = le.transform(test_df[c])\n",
    "\n",
    "    # Create a new column combining the encoded value and weight capacity\n",
    "    new_col = f\"{c}_Weight_Capacity_Combined\"\n",
    "    train_df[new_col] = train_df[c] * 100 + train_df[\"Weight Capacity (kg)\"]\n",
    "    test_df[new_col] = test_df[c] * 100 + test_df[\"Weight Capacity (kg)\"]\n",
    "\n",
    "    # Append the new column name to the combined_list list\n",
    "    combined_list.append(new_col)\n",
    "\n",
    "print(f\"We now have {len(combined_list)} new columns\")\n",
    "print(combined_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_variables = cat_columns + num_columns + combined_list\n",
    "print(f\"We now have {len(input_variables)} columns:\")\n",
    "print(input_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Features (feture_distance)\n",
    "- `feature_Distance` is a new feature that captures the distance between the backpack and the user.\n",
    "- It is created by taking the absolute difference between the weight capacity and the average weight capacity of the backpacks in the dataset.\n",
    "- This feature is useful because it captures the distance between the backpack and the user, which can be important for predicting the price of the backpack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be used for distance calculations\n",
    "distance_columns = ['Brand', 'Material', 'Size', 'Style', 'Color']\n",
    "\n",
    "# Ensure the columns are encoded numerically\n",
    "label_encoders = {}\n",
    "for col in distance_columns:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0)\n",
    "    le.fit(combined)\n",
    "    label_encoders[col] = le\n",
    "    train_df[col] = le.transform(train_df[col])\n",
    "    test_df[col] = le.transform(test_df[col])\n",
    "\n",
    "# Function to calculate distance features\n",
    "def calculate_distance_features(df, columns):\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col1, col2 = columns[i], columns[j]\n",
    "            new_col = f\"{col1}_{col2}_Distance\"\n",
    "            df[new_col] = np.sqrt((df[col1] - df[col2]) ** 2)\n",
    "    return df\n",
    "\n",
    "# Apply the function to train and test datasets\n",
    "train_df = calculate_distance_features(train_df, distance_columns)\n",
    "test_df = calculate_distance_features(test_df, distance_columns)\n",
    "\n",
    "# Display the new columns added\n",
    "distance_features = [col for col in train_df.columns if '_Distance' in col]\n",
    "print(f\"Distance features added: {distance_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Aggregations\n",
    "agg_features = {}\n",
    "group_cols = ['Weight Capacity (kg)'] + combined_list  # Group by Weight Capacity and combo features\n",
    "agg_funcs = ['mean', 'std', 'min', 'max', 'median', 'count', 'skew']\n",
    "\n",
    "for col in group_cols:\n",
    "    agg = train_df.groupby(col)['Price'].agg(agg_funcs).reset_index()\n",
    "    agg.columns = [col] + [f\"{col}_Price_{func}\" for func in agg_funcs]\n",
    "    train_df = train_df.merge(agg, on=col, how='left')\n",
    "    test_df = test_df.merge(agg, on=col, how='left')\n",
    "    agg_features.update({col: agg.columns[1:]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Grouping operations were performed to compute statistical metrics such as mean, standard deviation, min, max, median, count, and skew across various feature combinations. Aggregations were applied based on `Weight Capacity (kg)` and the features in the combined_list, helping to uncover meaningful interactions in the data.\n",
    "\n",
    "Additionally, Target Encoding was applied to the columns in base_features. Each categorical value was replaced with the smoothed average Price within its category, enabling the model to better capture the relationship between categorical features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding\n",
    "base_features = ['Brand', 'Material', 'Size', 'Style', 'Color']\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for col in base_features:\n",
    "\n",
    "    # Initialize the column with a compatible data type (float64)\n",
    "    train_df[f\"{col}_TE\"] = 0.0  # Explicitly set to float64\n",
    "\n",
    "    for train_idx, val_idx in kf.split(train_df):\n",
    "        train_fold, val_fold = train_df.iloc[train_idx], train_df.iloc[val_idx]\n",
    "        means = train_fold.groupby(col)['Price'].mean()\n",
    "        train_df.loc[val_idx, f\"{col}_TE\"] = val_fold[col].map(means)\n",
    "        \n",
    "    test_df[f\"{col}_TE\"] = test_df[col].map(train_df.groupby(col)['Price'].mean())\n",
    "\n",
    "# Fill NaN values in test_df after mapping\n",
    "test_df.fillna(test_df.mean(), inplace=True)\n",
    "\n",
    "print(\"Group Aggregations and Target Encoding completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature engineering - interaction terms\n",
    "# train_encoded_df[\"Price_per_Compartment\"] = train_encoded_df[\"Price\"] / (train_encoded_df[\"Compartments\"] + 1)\n",
    "# train_encoded_df[\"Has_Laptop_Compartment\"] = train_encoded_df[\"Laptop Compartment\"].apply(lambda x: 1 if x == \"Yes\" else 0)\n",
    "\n",
    "# # standardization (scale numerical features)\n",
    "# scaler = StandardScaler()\n",
    "# train_encoded_df[num_cols] = scaler.fit_transform(train_encoded_df[num_cols])\n",
    "# test_encoded_df[num_cols] = scaler.transform(test_encoded_df[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check to see the changes\n",
    "# train_encoded_df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_encoded_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets save the data to a new csv file under processed_data folder.\n",
    "\n",
    "Provided the code to save the data to a new csv file under processed_data folder. However, the size of the file is too large to be uploaded to GitHub. So, for now, we will not upload the processed data and will use the raw data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the processed data to the appropriate location\n",
    "# if on_colab:\n",
    "#     # Save processed data to Google Drive\n",
    "#     train_fold.to_csv(os.path.join(processed_path, \"train_fold.csv\"), index=False)\n",
    "#     val_fold.to_csv(os.path.join(processed_path, \"val_fold.csv\"), index=False)\n",
    "#     print(\"Processed train and validation folds saved to Google Drive.\")\n",
    "# else:\n",
    "#     # Check if the directory exists, if not, create it\n",
    "#     processed_file_path = '../data/processed'\n",
    "#     if not os.path.exists(processed_file_path):\n",
    "#         os.makedirs(processed_file_path)\n",
    "\n",
    "#     # Save the processed train and validation data locally\n",
    "#     train_fold.to_csv(os.path.join(processed_file_path, \"train_fold.csv\"), index=False)\n",
    "#     val_fold.to_csv(os.path.join(processed_file_path, \"val_fold.csv\"), index=False)\n",
    "#     print(\"Processed train and validation folds saved to local drive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Checks to see if this notebook is running on Google Colab and save to the recommended locations. \n",
    "# if on_colab:\n",
    "#     # save processed data to drive\n",
    "#     train_encoded_df.to_csv(os.path.join(processed_path, \"train_final.csv\"), index=False)\n",
    "#     test_encoded_df.to_csv(os.path.join(processed_path, \"test_final.csv\"), index=False)\n",
    "#     print(\"Feature engineering completed - processed files saved on Google Drive.\")\n",
    "# else:\n",
    "#     # Check if the directory exists, if not, create it\n",
    "#     processed_file_path = '../data/processed'\n",
    "#     if not os.path.exists(processed_file_path):\n",
    "#         os.makedirs(processed_file_path)\n",
    "\n",
    "#     # Save the transformed training data\n",
    "#     train_encoded_df.to_csv(processed_file_path + '/train_processed.csv', index=True)\n",
    "#     processed_train_encoded_df = train_encoded_df.copy()\n",
    "\n",
    "#     # Save the transformed testing data\n",
    "#     test_encoded_df.to_csv(processed_file_path + '/test_processed.csv', index=True)\n",
    "#     processed_test_encoded_df = test_encoded_df.copy()\n",
    "#     print(\"Feature engineering completed - processed files saved to Non-Colab Environment Local Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations so far ### \n",
    "\n",
    "TODO:\n",
    "Delete this section\n",
    "\n",
    "- Merged train and train_extra datasets\n",
    "- Dropped id column\n",
    "- OHE for brand, color and style\n",
    "- Performed label encoding for material and size columns\n",
    "- Added feature interactions\n",
    "  - price_per_compartment, has_laptop_compartment\n",
    "- mode imputation for categorical columns missing vals\n",
    "- forward fill for numerical columns missing vals\n",
    "- saved processed data to processed_data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling\n",
    "\n",
    "In this section, we will select and train machine learning models to predict the price of the backpack.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_path = '../data/processed'\n",
    "\n",
    "if on_colab:\n",
    "    train_final_df = pd.read_csv(os.path.join(processed_path, \"train_final.csv\"))\n",
    "    test_final_df = pd.read_csv(os.path.join(processed_path, \"test_final.csv\"))\n",
    "\n",
    "    print(\"Train Final Shape:\", train_final_df.shape)\n",
    "    print(\"Test Final Shape:\", test_final_df.shape)\n",
    "\n",
    "    display(train_final_df.head(3))\n",
    "    display(test_final_df.head(3))\n",
    "else:\n",
    "    train_final_df = pd.read_csv(os.path.join(processed_file_path, \"train_processed.csv\"))\n",
    "    test_final_df = pd.read_csv(os.path.join(processed_file_path, \"test_processed.csv\"))\n",
    "\n",
    "    print(\"Train Final Shape:\", train_final_df.shape)\n",
    "    print(\"Test Final Shape:\", test_final_df.shape)\n",
    "\n",
    "    display(train_final_df.head())\n",
    "    display(test_final_df.head())\n",
    "\n",
    "# Drop the 'id' and 'Unnamed: 0' columns from the datasets\n",
    "train_final_df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "test_final_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Verify the changes\n",
    "print(train_final_df.head())\n",
    "print(test_final_df.head())\n",
    "\n",
    "train_final_df.columns\n",
    "\n",
    "# Ensure 'Price' column is present before dropping\n",
    "if 'Price' in train_final_df.columns:\n",
    "\t# Select the target feature\n",
    "\ty = train_final_df['Price']\n",
    "\n",
    "\t# Drop the target feature from the training dataframe\n",
    "\tX = train_final_df.drop(columns=['Price'])\n",
    "\n",
    "\t# Verify the changes\n",
    "\tprint(X.head())\n",
    "\tprint(y.head())\n",
    "else:\n",
    "\tprint(\"'Price' column is not present in train_final_df\")\n",
    "\n",
    "# Check the shape of X, and y\n",
    "print(f\"X Shape: {X.shape}\")\n",
    "print(f\"y Shape: {y.shape}\")\n",
    "\n",
    "# leftover cat cols\n",
    "obj_cols = X.select_dtypes(include=['object']).columns\n",
    "print(\"Object columns:\", obj_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train-Test Split\n",
    "\n",
    "First, let's split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# convert cat to numerical\n",
    "X_train[\"Laptop Compartment\"] = X_train[\"Laptop Compartment\"].map({\"Yes\": 1, \"No\": 0})\n",
    "X_train[\"Waterproof\"] = X_train[\"Waterproof\"].map({\"Yes\": 1, \"No\": 0})\n",
    "X_val[\"Laptop Compartment\"] = X_val[\"Laptop Compartment\"].map({\"Yes\": 1, \"No\": 0})\n",
    "X_val[\"Waterproof\"] = X_val[\"Waterproof\"].map({\"Yes\": 1, \"No\": 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model Selection\n",
    "\n",
    "Next, let's select a model to train on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model\n",
    "lr_model = LinearRegression().fit(X_train, y_train)\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the XGBoost model\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=1_000,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.9,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=10,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "xgb_model.fit(\n",
    "    train_fold.drop(columns=['Price']), \n",
    "    train_fold['Price'], \n",
    "    eval_set=[(val_fold.drop(columns=['Price']), val_fold['Price'])]\n",
    ")\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "# Calculate RMSE for the XGBoost model\n",
    "rmse_xgb = root_mean_squared_error(y_val, y_pred_xgb, squared=False)\n",
    "print(\"Baseline XGBoost RMSE:\", rmse_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "In this section, we will evaluate the performance of our models using various metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our y_hat value with y_pred_lr variable.\n",
    "y_pred_lr = lr_model.predict(X_val)\n",
    "\n",
    "rmse_lr = root_mean_squared_error(y_val, y_pred_lr)\n",
    "print(\"Baseline Linear Regression RMSE:\", rmse_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals\n",
    "residuals = y_val - y_pred_lr\n",
    "\n",
    "# Plot baseline and residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(y_val, y_pred_lr, alpha=0.3)\n",
    "plt.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Baseline Model: Predicted vs Actual Price')\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of residuals\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(residuals, kde=True, bins=30)\n",
    "plt.axvline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "#### Residual Distribution\n",
    "- There is a large spike near residual = 0, indicating that, on average, the baseline model predictions are close to the actual price for many backpacks.\n",
    "- Wide spread, heavy tails: Residuals extend from about -60 to +60, suggesting that for some backpacks, the baseline model underpredicts by up to 60 units and overpredicts by up to 60 units. A high variance in residuals suggests significant differences in how well the linear baseline model captures the data across different price ranges/categories.\n",
    "- RMSE is very high: 25.03. While many predictions cluster around 0, there is still a lot of deviation.\n",
    "\n",
    "#### Predicted vs Actual Price\n",
    "- Points spread out well above and below the diagonal, meaning the baseline model error is not constant across the price range. The baseline model may overestimate some backpack prices (points above the red line) and underestimate others (points below the red line).\n",
    "- There are multiple lines in the predicted values, indicating that the linear baseline model is grouping certain types of backpacks (perhaps by brand, color, or other splits) and producing similar predictions for them. This effect is more pronounced in linear baseline models when strong interactions are not fully captured.\n",
    "\n",
    "#### Overall\n",
    "- The baseline captures some positive relationship, but there is significant variance in the predictions with clear clusters at certain price ranges. The high error spread suggests the relationship cannot be captured by a simple linear baseline model and certain features may be missing.\n",
    "- The multiple lines indicate that combining certain features leads to the same predicted price, so we may explore adding more interaction terms or exploring a non-linear baseline model next, such as a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Model Optimization\n",
    "\n",
    "In this section, we will optimize the hyperparameters of our models to improve their performance.\n",
    "\n",
    "\n",
    "## 6. Final Submission\n",
    "\n",
    "In this section, we will select the best model and make final predictions on the test set.\n",
    "\n",
    "\n",
    "## 7. Conclusion\n",
    "\n",
    "In this section, we will summarize our findings and discuss the implications of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
